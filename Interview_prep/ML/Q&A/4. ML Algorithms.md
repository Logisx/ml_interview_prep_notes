![Pasted image 20250313113023.png](Pasted%20image%2020250313113023.png)
![Pasted image 20250313113456.png](Pasted%20image%2020250313113456.png)

---

![Pasted image 20250313113515.png](Pasted%20image%2020250313113515.png)
![Pasted image 20250313113809.png](Pasted%20image%2020250313113809.png)

---

![Pasted image 20250313113824.png](Pasted%20image%2020250313113824.png)
Decision Trees, Random Forests, Gradient Boosting Machine, Support Vector Machine, KNearest Neighbors, . . .

---

![Pasted image 20250313114012.png](Pasted%20image%2020250313114012.png)
![Pasted image 20250313114156.png](Pasted%20image%2020250313114156.png)
![Pasted image 20250313114459.png](Pasted%20image%2020250313114459.png)
![Pasted image 20250313114508.png](Pasted%20image%2020250313114508.png)![Pasted image 20250313114512.png](Pasted%20image%2020250313114512.png)

---

![Pasted image 20250313115045.png](Pasted%20image%2020250313115045.png)
![Pasted image 20250313114947.png](Pasted%20image%2020250313114947.png)
Also, silhoette cefficient may be used.
ii. If the labels are known, performance can be evaluated using **accuracy, precision, recall, and F1-score** after assigning cluster labels to true labels using a **confusion matrix**.
iii. If labels arenâ€™t known, performance can be assessed using **silhouette score, Davies-Bouldin index, or Calinski-Harabasz index**, which measure cluster cohesion and separation.
![Pasted image 20250313115027.png](Pasted%20image%2020250313115027.png)
![Pasted image 20250313115033.png](Pasted%20image%2020250313115033.png)

---

![Pasted image 20250313115217.png](Pasted%20image%2020250313115217.png)
Common practice is is to start with k = âˆš N, where N is the number of data points.
![Pasted image 20250313115551.png](Pasted%20image%2020250313115551.png)

![Pasted image 20250313115615.png](Pasted%20image%2020250313115615.png)![Pasted image 20250313115623.png](Pasted%20image%2020250313115623.png)

---

![Pasted image 20250313115923.png](Pasted%20image%2020250313115923.png)
![Pasted image 20250313115914.png](Pasted%20image%2020250313115914.png)

---

![Pasted image 20250313115951.png](Pasted%20image%2020250313115951.png)![Pasted image 20250313120227.png](Pasted%20image%2020250313120227.png)It's generally more common to see boosting used in traditional models like decision trees rather than pure deep neural networks as the networks are long to train, so bagging/boosting will take too much time.
![Pasted image 20250313120308.png](Pasted%20image%2020250313120308.png)

---

![Pasted image 20250313120326.png](../../../../attachments/interview_prep/3c2959f8b769c8eb8e5b9a69a37a3dcf.png)
![Pasted image 20250313120537.png](40687f4b8632ad30897df5889cfdea9f.png)
![Pasted image 20250313120542.png](9983c8aa206c95504e8f828bce635739.png)The **adjacency matrices** of two **isomorphic graphs** will be **permutations of each other**. This means that there exists a **reordering of the rows and columns** of one graphâ€™s adjacency matrix that results in the adjacency matrix of the other graph.

---

![Pasted image 20250313120618.png](a68f0cd85b3397e058545e06b4d09f0e.png)![Pasted image 20250313121025.png](3d1c806d2c7c46041beaa9869d207e19.png)
![Pasted image 20250313121030.png](d115bb5df5380c1238ca4cf390136544.png)

---

![Pasted image 20250313121116.png](97872db50b683783c417beb54ac6a32f.png)
Yes, feature scaling is generally necessary for kernel methods, such as Support Vector Machines (SVMs) with a kernel, because these methods rely on calculating the similarity between data points using a kernel function. Most kernel functions, like the radial basis function (RBF), measure distances (e.g., Euclidean distance) or angles (e.g., cosine similarity) between points in the feature space.

If the features have very different scales (e.g., one feature ranges from 0 to 1 and another ranges from 1,000 to 1,000,000), the kernel might give more weight to features with larger ranges, leading to biased results. Feature scaling (such as normalization or standardization) ensures that all features contribute equally to the kernel computation, improving model performance and stability.

---

ðŸš©ðŸš©ðŸš©ðŸš©ðŸš©ðŸš©ðŸš©ðŸš©
![Pasted image 20250313132611.png](1c4f169aad2bbc1140a6f5c2bd8357a8.png)
The Naive Bayes classifier is considered "naive" because it assumes that the features (in this case, the words in the tweet) are conditionally independent of each other, meaning that the presence or absence of a word doesn't affect the presence or absence of another word, given the class label. This simplification makes the computation easier but doesn't always capture the true relationships between words.

---

![Pasted image 20250313132751.png](3ebac6be86f92367448c8aace68add5c.png)![Pasted image 20250313133021.png](a651306416601d390a995a185239a92b.png)
Gradient boosting is particularly effective for problems involving structured/tabular data, like classification and regression tasks. It excels in scenarios where relationships between features are complex and non-linear, and it's well-suited for imbalanced datasets. It is also known for handling missing data well. **Gradient boosting** models, such as **XGBoost** and **LightGBM**, are generally considered **less interpretable** than simpler models like linear regression or decision trees due to their complexity.

---

![Pasted image 20250313133043.png](e3d6d81a3c1db81e36c4b60f2a7808c0.png)
Two sets are linearly separable if there exists at least one hyperplane that splits the space in such a way that all of the points of the first set lie on one one side of the hyperplane, and all of the points of the second set lie on the other side. In the linearly separable case, SVM is trying to find the hyperplane that maximizes the margin, with the condition that both classes are classified correctly. But in reality, datasets are almost never linearly separable, so the condition of 100% correctly classified by a hyperplane will never be met.
![Pasted image 20250313133233.png](269f857ce6f54352342890385b65b527.png)
![Pasted image 20250313133238.png](6eca4a124dd26d0b5a8df392bbc6ca44.png)
iv. How well would vanilla SVM work on this dataset (see Figure 18)? Answer: SVM wonâ€™t work at all, since the two classes are not linearly separable. As discussed above, two potential solutions are: introducing slack variables; using non-linear kernels.