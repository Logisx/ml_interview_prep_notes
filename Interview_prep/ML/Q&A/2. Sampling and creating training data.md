![Pasted image 20250308175938.png](../../../attachments/Pasted%20image%2020250308175938.png)
Logically, 6 * 5 * 4 = 120. But first Shirt A and then Shirt B is the same as Shirt B then A. So, order doesn't matter, so we divide 120 by 2 and get 60. Answer is 60.
It can be done via the combination formula.
![Pasted image 20250308180122.png](../../../attachments/Pasted%20image%2020250308180122.png)

---

![Pasted image 20250308180149.png](../../../attachments/Pasted%20image%2020250308180149.png)
In **sampling with replacement**, each selected data point is returned to the dataset and can be chosen again in subsequent draws. 
In **sampling without replacement**, once a data point is selected, it is not available for future selections.
- **With replacement**: **Bootstrap sampling** in bagging (e.g., Random Forest) to create diverse training sets for each tree.
- **Without replacement**: **Cross-validation**, where each fold should have unique data points to ensure proper model evaluation.
Nevertheless, this argument is mostly regarding small population sizes. In case of large datasets, both samplings are nearly identical.

---

🚩🚩🚩🚩🚩🚩🚩🚩
![Pasted image 20250308180614.png](../../../attachments/Pasted%20image%2020250308180614.png)

---

🚩🚩🚩🚩🚩🚩🚩🚩
![Pasted image 20250308181106.png](../../../attachments/Pasted%20image%2020250308181106.png)

---

🚩🚩🚩🚩🚩🚩🚩🚩
![Pasted image 20250308181234.png](../../../attachments/Pasted%20image%2020250308181234.png)

---

![Pasted image 20250308181927.png](../../../attachments/Pasted%20image%2020250308181927.png)
https://www.cloudresearch.com/resources/guides/sampling/pros-cons-of-different-sampling-methods/
Answer: Before we start sampling, we state a hypothesis that comments violating the website’s rule over the past 24 months vary highly, and is to a high degree dependent on the predominant world events happening at the time (e.g. election campaigns, the pandemic, social movements, etc.). Therefore, in order to build a robust classifier capable of generalizing to unseen comments (and events related to them) in the future, we find it important to have representative samples from each historical period. One such approach that ensures the above-mentioned property is stratified sampling. In particular, we select each month to represent a single strata, and then sample randomly within this population.

Answer: Given that we have 20 annotators labeling 100K comments, in order to estimate the quality, we could look at 5% of the comments (5000 out of the 100K), or roughly one annotator’s workload. Since we are only interested in estimating the overall quality of the sample, we could perform simple random sampling. On the other hand, if we were interested in the quality of work of each annotator, we could again perform stratified sampling, where each annotator’s labeled pool of comments is considered a strata. From there, we can make a more inform

![Pasted image 20250308183856.png](../../../attachments/Pasted%20image%2020250308183856.png)

---

![Pasted image 20250308181947.png](../../../attachments/Pasted%20image%2020250308181947.png)
Selection bias is the bias introduced by the selection of individuals, groups, or data for analysis in such a way that proper randomization is not achieved, thereby failing to ensure that the sample obtained is representative of the population intended to be analyzed. It might be the case that the 1% of translated articles are not a random sample from the population of all articles, but only ones containing news appealing to readers in China or the best articles overall. Therefore, if the news site starts putting in more resources in translating articles from the entire population (meaning not only ones that could be considered of interest to Chinese readers), the return of investment might actually not pay off, since not all articles might be of interest to the readers in China.

---

![Pasted image 20250308184802.png](../../../attachments/Pasted%20image%2020250308184802.png)![Pasted image 20250310100023.png](../../../attachments/Pasted%20image%2020250310100023.png)

---

![Pasted image 20250310100040.png](../../../attachments/Pasted%20image%2020250310100040.png)
There are various statistical methods, but the most prominent approach is to train your model with progressively larger subsets of data and plot performance (accuracy, loss, etc.). If the performance plateaus, more data may not help.
![Pasted image 20250310110307.png](../../../attachments/Pasted%20image%2020250310110307.png)

---

![Pasted image 20250310110330.png](../../../attachments/Pasted%20image%2020250310110330.png)
Many anomaly detection techniques have been proposed in the literature, some of which are:
- Statistical. A common approach is to assume Normality of the data and calculate the Z-score. Then, given a certain threshold (e.g. 3), mark all samples above the threshold as outliers; in this example, we essentially label points that are 3 standard deviations away from the mean as anomalies. 
- Density-based techniques. A prominent example is to use k-NN in such a way that for each point one computes the sum of the distances of the k nearest neighbors, denoted as weight. Then, outliers are points that have the largest weight.
You can use Inter-quartile ranges, visualization etc to find outliers. 

What to do with it depends on your case. If it's an error, you should remove it. If it's rare but possible you should keep it. Maybe you even want to detect the outliers in your algorithm (spam emails).
![Pasted image 20250310111228.png](../../../attachments/Pasted%20image%2020250310111228.png)

1.) Are these outliers due to errors in the data collection? If so, you can remove them from the analysis by z-scoring and thresholding.

2.) Are these outliers legitimate? Is it okay if you don't catch them? Use regularization.

3.) Is it critical that you catch these outliers? Consider using:

3a. Anomaly detection model to catch the outliers, then run them through a separate classifier.

3b. Heavily weight outliers in the single classifier.

3c. Use a very robust model.

---

![Pasted image 20250310111243.png](../../../attachments/Pasted%20image%2020250310111243.png)
![Pasted image 20250310111842.png](../../../attachments/Pasted%20image%2020250310111842.png)
![Pasted image 20250310111850.png](../../../attachments/Pasted%20image%2020250310111850.png)

So basically don't remove duplicates if it's natural to have them in your data. 
If all data points are duplicated it probably won't affect performance of metric, but training time increases.

---

![Pasted image 20250310112034.png](../../../attachments/Pasted%20image%2020250310112034.png)
![Pasted image 20250310112541.png](../../../attachments/Pasted%20image%2020250310112541.png)

By far, the most common means of dealing with missing data is listwise deletion (also known as complete case), which is when all cases with a missing value are deleted. If the data are missing completely at random, then listwise deletion does not add any bias, but it does decrease the power of the analysis by decreasing the effective sample size. 

If the cases are not missing completely at random, then listwise deletion will introduce bias because the sub-sample of cases represented by removing the missing data are not representative of the original sample (and if the original sample was itself a representative sample of a population, the complete cases are not representative of that population either). While listwise deletion is unbiased when the missing data is missing completely at random, this is rarely the case in reality. 

One way of preventing the introduction of this bias is to use an imputation technique. This way, we can retain the discussed samples and thereby not perpetuate the sampling bias. Another option is to use robust models (rand forest) as it handles missing values better.

---

![Pasted image 20250310112941.png](../../../attachments/Pasted%20image%2020250310112941.png)Randomization is crucial in experimental design because it helps ensure **unbiased, reliable, and generalizable results**. Here’s why:

### **1. Eliminates Selection Bias**

- Ensures that treatment and control groups are comparable.
- Prevents researchers from unintentionally assigning subjects in a way that influences results.

### **2. Controls for Confounding Variables**

- Distributes unknown and known confounding factors evenly across groups.
- Reduces the risk that external factors skew the results.

### **3. Enables Valid Statistical Inference**

- Justifies the use of statistical tests like **t-tests and ANOVA**, which assume random sampling.
- Increases the likelihood that observed effects are **due to the treatment, not hidden biases**.

### **4. Enhances Reproducibility & Generalizability**

- Ensures results aren’t specific to one sample.
- Makes findings more applicable to the real-world population.

### **5. Reduces the Risk of Overfitting (in ML Experiments)**

- Prevents models from learning dataset-specific patterns that don’t generalize.
- Helps create unbiased training, validation, and test splits.


---

![Pasted image 20250310113311.png](../../../attachments/Pasted%20image%2020250310113311.png)
  APPLY ONLY TO THE TRAINING SET!!!!!
  
There's typically two things I do differently:

1. I weight the samples according to some objective measure of importance (eg If we're trying to detect fraud, I'll use the relative cost of a false positive vs a false negative to calculate the weights).
    
2. I use metrics that take the imbalance into consideration and work to optimise those. For instance, I'll use kappa instead of accuracy, and work to ensure we can get a high kappa level before using the model.
    

With that said, I haven't found imbalanced classes to be a huge problem. It makes the problem more difficult to model, but not massively so.


For training you have 4 real options:

1. Use class weights modify your loss function, these are available in Keras but i am not sure what library you are using. I used to do this, but you end up with a very variable learning rate for batches that contain some of minority class.
    
2. Under-sample your majority classes so that the dataset is balanced, the issue here is you throw away potentially useful data, but at least training is a bit faster.
    
3. Over-sample your minority class, this is my new favourite as every example will have the same learning rate and you do not discard data. The only downside is you will have more data so training will take longer.
    
4. Do nothing. Is your imbalance is not large you might get away with it, but best to avoid this.
    

For model evaluation avoid looking at accuracy, instead look at precision & recall and the f1-score. Confusion matrices and ROC curves are alway good as well and will help you avoid the trap of 99% accuracy on a 1:100 class imbalance.
![Pasted image 20250310150250.png](../../../attachments/Pasted%20image%2020250310150250.png)![Pasted image 20250310150304.png](../../../attachments/Pasted%20image%2020250310150304.png)
![Pasted image 20250310150926.png](../../../attachments/Pasted%20image%2020250310150926.png)

---

![Pasted image 20250310152838.png](../../../attachments/Pasted%20image%2020250310152838.png)
1)
By oversampling when training offline, we force the model to put more weight on the rare class, thereby increasing the recall, but decreasing the precision.

However, if the rare class happens truly rarely even in production (IID assumption), then the results that we obtain on our test set are not at all representative of the results that we hope to expect in production, since we oversample before the train/test split and wrongfully introduce a shift between the test set and the data seen during production.

A better approach to the problem is to only oversample the train set after we have done the train/test split – this way, we don’t introduce a distribution shift in the test set with respect to the data seen during production, thereby obtaining more representative results.

Also, it may be something related to oversampling itself: if your oversampled samples are not representative of what you may find in a real world probably you need more data.

🔹 **Fix:** Always split your data into train and test sets **before** applying oversampling techniques to avoid leakage.

2)
By randomly splitting data collected over a 7-day period, you risk **temporal leakage**—comments from the same users, conversations, or trends might appear in both train and test sets. If a spam attack happens on Day 5, similar spam messages from the same spam campaign might be present in both splits, making the test set **too easy** and inflating performance metrics.

🔹 **Fix:** Instead of random splitting, use **time-based splits** (e.g., train on Days 1–5, test on Days 6–7) to better simulate real-world deployment where future data is unseen at training time.

---

![Pasted image 20250310153151.png](../../../attachments/Pasted%20image%2020250310153151.png)
![Pasted image 20250310153751.png](../../../attachments/Pasted%20image%2020250310153751.png)
![Pasted image 20250310153808.png](../../../attachments/Pasted%20image%2020250310153808.png)

---

![Pasted image 20250310154257.png](../../../attachments/Pasted%20image%2020250310154257.png)
Feature or column-wise leakage is caused by the inclusion of columns which are one of the following: a duplicate label, a proxy for the label, or the label itself. These features, known as anachronisms, will not be available when the model is used for predictions, and result in leakage if included when the model is trained. For example, including a ”MonthlySalary” column when predicting ”YearlySalary”; or ”MinutesLate” when predicting ”IsLate”; or more subtly ”NumOfLatePayments” when predicting ”ShouldGiveLoan”.

Do normalization correctly (AFTER splitting) 

Normalization can also help when features have different scales, which might cause models to focus more on certain features during training. By normalizing features, you prevent a model from disproportionately focusing on larger-scale features, which might cause overfitting, potentially leading to **indirect leakage**.

iii. How do you detect feature leakage? 
Answer: Several cases that can indicate potential feature leakage: 
- The model performance is too good to be true. 
- A feature has abnormally high correlation with the target value. 
- After training, the model weights for a given feature are significantly higher than the others.

---

![Pasted image 20250310155627.png](../../../attachments/Pasted%20image%2020250310155627.png)
The main problem with randomly shuffling the data and partitioning it into training, validation, and test sets is the potential for **temporal leakage**. Since you're working with tweets from the past 24 months, the data has a time component. If you randomly shuffle the tweets, tweets from earlier months may end up in the test or validation set, while more recent ones might be in the training set. This could cause the model to inadvertently learn from future data, making the evaluation results less reliable and potentially leading to **overfitting** or **data leakage**.

To avoid this, you should consider using a **time-based split**, where you partition the data based on time. For example, you could use tweets from the first 20 months for training, the next 2 months for validation, and the final 2 months for testing. This way, the model is only trained on past data and evaluated on data it has never seen before, simulating a real-world scenario where information from the future isn't available during training.

---

![Pasted image 20250310155939.png](../../../attachments/Pasted%20image%2020250310155939.png)
In general, there are 2 main types of features in a tabular dataset: continuous and categorical. Typically, one does not apply intricate pre-processing techniques for the continuous variables, except for normalization/standardization. 

Categorical variables on the other hand are a bit more intricate and require special care. Neural nets cannot handle textual features, so we have to transform them to numerical ones. However, simply enumerating the categories is plain wrong – if you represent ”apples” with 1, and ”oranges” with 2, does it mean that ”oranges” = 2 x ”apples”? Another way to encode the categorical features is with one-hot encoding, but this can introduce data sparsity, which can be an undesired trait of our dataset as we have seen in a previous question. 

Therefore, one of the most widely used methods for encoding textual features is to use word embeddings, such as Word2Vec. The benefits of using an embedding is that it provides low-dimensional, distributed representation that allow for capturing relationships between the categories (eg. ”king” - ”man” + ”woman” = ”queen”)

![Pasted image 20250310160131.png](../../../attachments/Pasted%20image%2020250310160131.png)

---

![Pasted image 20250310160146.png](../../../attachments/Pasted%20image%2020250310160146.png)
**Training Error:**

- **Decrease**: As you add more features, the model becomes more flexible and has more information to fit the data. This flexibility can help the model capture more intricate patterns in the training data, leading to a **lower training error**. The model may fit the training data perfectly or nearly perfectly, especially if the features are highly informative.

**Test Error:**

- **Increase**: While the training error might decrease, the **test error** can increase. This is due to the **curse of dimensionality**. As the number of features grows, the data becomes sparser in higher-dimensional space. This sparsity makes it harder for the model to generalize well to unseen data, leading to overfitting. Essentially, the model starts to "memorize" the training data instead of learning the underlying patterns, which results in poor performance on the test set.

In summary:

- **Training error** could decrease, as the model has more features to work with.
- **Test error** could increase, due to overfitting and the curse of dimensionality.
