## Self-Attention

Attention means that we apply weights to inputs to make the input be more useful by adding context to it.
![Pasted image 20250324175539.png](ml_interview_prep_notes/Interview_prep/DL/attachments/Pasted%20image%2020250324175539.png) processing we cannot just use "give more attention to words close to the current word" as language doesn't depend on proximity. So we want to find a way to get weights that will add context to our inputs. (t-token, v-embedding vector, y-input wit![Pasted image 202503241![Pasted image 20250324175805.png](../../../DL/attachments/Pasted%20image%2020250324175805.png)rd embedding has some meaning coded in the value in each position. Words that have a similarity are more likely to be connected in context. If we take dot product of 2 vectors it will show us how similar those are and we can use i![Pasted image 202503241![Pasted image 20250324180051.png](ml_interview_prep_notes/Interview_prep/DL/attachments/Pasted%20image%2020250324180051.png)a new vector for a word input, that now contains info about the word relation to ea![Pasted image 202503241![Pasted image 20250324180438.png](../../../DL/attachments/350706912c2d1f2f9c788300984b177d.png)ing weights (Queries, Keys and Values)

Basically here is a flow chart of the process that we repeat for each word (here an example for the 3rd word). But we have no trainable element here and maybe if we add it we can get better results. (S - stands for scores (non normali![Pasted image 202503241![Pasted image 20250324180848.png](ml_interview_prep_notes/Interview_prep/DL/attachments/dbb3062421fab07e1ceff2eacb2e770a.png)dd weights to each part where we use our word vectors. To differentiate them we call those parts Query, ![Pasted image 202503241![Pasted image 20250324181108.png](../../../DL/attachments/5e73d9891ce1343fd947216114e2bd84.png)art of the process with weights. Linear layers are just for adding weights, there are no bias terms. Red lines are backpropagation. We can stack those "self attention" blocks toge![Pasted image 202503241![Pasted image 20250324181311.png](ml_interview_prep_notes/Interview_prep/DL/attachments/01f1466f063d8723b8f0a38190c3fd74.png)ti-head Attention

We don't have to oversaturate one attention mechanism if we can spread the "cognitive" load on ![Pasted image 202503241![Pasted image 20250324181831.png](../../../DL/attachments/00bde92759c5b15dd3baf823a373b131.png)run those attentions in parallel and each will focus on different meanings and contexts. (h - for "head")
We had to add a concat + dense layer in the end to "sum up" the new embe![Pasted image 202503241![Pasted image 20250324182032.png](ml_interview_prep_notes/Interview_prep/DL/attachments/ee731a482ab13e926168f31e19b8f015.png)stack those blocks and do kinda deep attention (i see a strong correlation with neural nets here). The right side is just a ![Pasted image 202503241![Pasted image 20250324182246.png](../../../DL/attachments/542a546a0f5c692b0dbeeaf97b22674d.png)nsformers

Going over "Attention is all you need". The left chart is SCALED self-attention (scaling by 1/![Pasted image 202503241![Pasted image 20250324182612.png](ml_interview_prep_notes/Interview_prep/DL/attachments/da479ff17ac6cf104827494bd67a790d.png)ginal paper was about machine translation. The left side is encoder, the right is decoder. For some other tasks we need only the encoder, so let's f![Pasted image 202503241![Pasted image 20250324182841.png](../../../DL/attachments/787d45841179a9161f9a1a5bb80cbfca.png)stack encoder block N times and use N as hyperparam. Number of heads is also![Pasted image 202503241![Pasted image 20250324183009.png](ml_interview_prep_notes/Interview_prep/DL/attachments/3bcba459420a0bd015971ccfe6f5acca.png)nal encoding is a way to add context on where the current word vector is located in a sequence. There multiple ways to do that: you can use some function or train the encoder. But important is that it is another hyperparam of the![Pasted image 202503241![Pasted image 20250324183738.png](../../../DL/attachments/1f200ce88bd31d4520f7647b33b06735.png)fits of transformers 
- Parallelism: in RNNs to compute each new state we need to know the results of previous 2. In multi-head attention we can compute heads in parallel as they are independent.
- No vanishing gradients problem: in RNNs the gradient flows to the starting states through many other states and vanishes. In multi-head attention it goes directly to each head and updates only 3 weights: keys, value![Pasted image 202503241![Pasted image 20250324184059.png](ml_interview_prep_notes/Interview_prep/DL/attachments/4b1e394f8f96435abcf1933389f31645.png)he machine tr![Pasted image 202503251![Pasted image 20250325155913.png](../../../DL/attachments/e00af110f119ba1b9b385bb317970105.png)![Pasted image 20250325155928.png](ml_interview_prep_notes/Interview_prep/DL/attachments/f5222175b726bebd9428ce3a91e28ae9.png)