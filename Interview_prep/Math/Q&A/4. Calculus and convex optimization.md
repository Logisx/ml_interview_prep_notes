![Pasted image 20250312144918.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312144918.png)is differentiable at a point if its derivative exists at that point. This means that the function is smooth (i.e., has no sharp corners or disco![Pasted image 202503121![Pasted image 20250312152515.png](../../../../Q&A/attachments/Pasted%20image%2020250312152515.png)1![Pasted image 20250312152731.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312152731.png)1![Pasted image 20250312152742.png](../../../../Q&A/attachments/Pasted%20image%2020250312152742.png)1![Pasted image 20250312152810.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312152810.png)2503121![Pasted image 20250312153216.png](../../../../Q&A/attachments/Pasted%20image%2020250312153216.png)–≥–Ω—É—Ç–∞—è like U
Concave - –≤—ã–ø—É–∫–ª–∞—è like n


Other cases:
y=x^3 neither convex nor concave globally, but concave for x<0 and convex>0.
linear functions are both convex and concave everywhere.

Any **local minimum** of a convex function is also a **global minimum**, making optimization easier and faster. Concavity works the same way, just flipped. Bad functions are non-convex non concave.

Convex optimization is important because it's the only type of optimization that we more or less understand. Some might argue that since many of the common objective functions in deep learning aren't convex, we don't need to know about convex optimization. However, even when the functions aren't convex, analyzing them as if they were convex often gives us meaningful bounds. If an algorithm doesn't work assuming that a loss function is convex, it definitely doesn't work when the loss function is non-convex.

Convexity is the exception, not the rule. If you're asked whether a function is convex and it isn't already in the list of commonly known convex functions, there's a good chance that it isn't convex.

Cross-entropy loss is convex because:  
‚úÖ It forms a **bowl-shaped** curve when plotted.  
‚úÖ The penalty grows **steeply** as the prediction moves away from the correct value.  
‚úÖ The rate of increase in loss never slows down, meaning it maintains an **upward curve**.

---

üö©üö©ÔøΩ![Pasted image 202503121![Pasted image 20250312162752.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312162752.png)it's just more efficient to compute gradients with Adam or SGD than do really fast and intelligent, but computation-hea![Pasted image 202503121![Pasted image 20250312163457.png](../../../../Q&A/attachments/Pasted%20image%2020250312163457.png)1![Pasted image 20250312164604.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312164604.png)![Pasted image 20250312163503.png](../../../../Q&A/attachments/Pasted%20image%2020250312163503.png)2503121![Pasted image 20250312164344.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312164344.png)1![Pasted image 20250312164516.png](../../../../Q&A/attachments/Pasted%20image%2020250312164516.png)2503121![Pasted image 20250312164625.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312164625.png)![Pasted image 20250312165032.png](../../../../Q&A/attachments/Pasted%20image%2020250312165032.png)![Pasted image 20250312165023.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312165023.png)2503121![Pasted image 20250312170707.png](../../../../Q&A/attachments/Pasted%20image%2020250312170707.png)1![Pasted image 20250312170659.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312170659.png)![Pasted image 202503121![Pasted image 20250312171339.png](../../../../Q&A/attachments/Pasted%20image%2020250312171339.png)![Pasted image 202503121![Pasted image 20250312171424.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250312171424.png)