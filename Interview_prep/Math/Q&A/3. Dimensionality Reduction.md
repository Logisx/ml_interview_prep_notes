
![Pasted image 20250308150033.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250308150033.png)1![Pasted image 20250308150150.png](../../../../Q&A/attachments/Pasted%20image%2020250308150150.png)![Pasted image 202503081![Pasted image 20250308150242.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250308150242.png)2503081![Pasted image 20250308150323.png](../../../../Q&A/attachments/Pasted%20image%2020250308150323.png)Component Analysis (PCA)** uses eigenvectors of the covariance matrix to project high-dimensional data onto lower dimensions while preservin![Pasted image 202503081![Pasted image 20250308150549.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250308150549.png)omponent Analysis (PCA), Spectral Clustering, Low rank factorization / rankk approximation of a matrix (eg. image compression), Estimating curvature of the loss through the eigenvalues of the Hessi![Pasted image 202503081![Pasted image 20250308150618.png](../../../../Q&A/attachments/Pasted%20image%2020250308150618.png)re interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. Since a change in height of one meter should be considered much more important than the change in weight of one kilogram, the previous assumption would be incorrect. Therefore, it is important to standardize the features before applying PCA (source: sci![Pasted image 202503081![Pasted image 20250308152710.png](ml_interview_prep_notes/Interview_prep/Math/Q&A/attachments/Pasted%20image%2020250308152710.png)osition is possible only for (square) diagonalizable matrices. On the other hand, the Singular Value Decomposition (SVD) always exists (even for non-square matrices).
SVD generalizes eigendecomposition.
PCA is SVD on a covariance ma![Pasted image 202503081![Pasted image 20250308152856.png](../../../../Q&A/attachments/Pasted%20image%2020250308152856.png)high-dimensional dataset and reduces/plots it as a low-dimensional graph, keeping a lot of information.

t-SNE (T-distributed Stochastic Neighbor Embedding) is a technique used to reduce high-dimensional data to two or three dimensions for visualization. It works by first calculating pairwise similarities between data points in high dimensions and then mapping them to a lower-dimensional space, preserving these similarities. 

We need t-SNE primarily for **data visualization**, helping to reveal patterns or clusters in complex, high-dimensional data, making it easier to understand and interpret. However, it’s mainly used for visualization and may not preserve global structures well.
  
Despite all the inaccuracies of t-SNE, it can be useful to visualise clusters in data. The same goes for text. If you want to see whether word embeddings actually group terms with similar meaning, t-SNE is your friend.